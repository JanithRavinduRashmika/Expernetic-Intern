{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mss import mss\n",
    "import cv2\n",
    "import pydirectinput\n",
    "import pytesseract\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import time\n",
    "\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,500,500), dtype=np.uint8)\n",
    "        self.action_space = Discrete(4)  # 0: up, 1: right, 2: down, 3: left\n",
    "\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':0, 'left':0, 'width':500, 'height':500}\n",
    "        self.done_location = {'top':250, 'left':175, 'width':150, 'height':50}\n",
    "        \n",
    "        self.start_pos = (1, 1)\n",
    "        self.goal_pos = (8, 8)\n",
    "        self.current_pos = self.start_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: 'up',\n",
    "            1: 'right',\n",
    "            2: 'down',\n",
    "            3: 'left'\n",
    "        }\n",
    "\n",
    "        pydirectinput.press(action_map[action])\n",
    "\n",
    "        done, done_cap = self.get_done()\n",
    "        new_observation = self.get_observation()\n",
    "\n",
    "        reward = 500 if done else -1\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        cv2.imshow('Game', np.array(self.cap.grab(self.game_location))[..., :3])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=225, y=275)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        observation = self.get_observation()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[..., :3]\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        channel = np.reshape(gray, (1,500,500))\n",
    "        \n",
    "        return channel\n",
    "    \n",
    "    def get_done(self):\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))\n",
    "\n",
    "        done_strings = [\"Play\"]\n",
    "\n",
    "        done = False\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done = True\n",
    "\n",
    "        return done, done_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=1000, learning_starts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:595] . unexpected pos 456852352 vs 456852304",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 862\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\zipfile.py:1169\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m-> 1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nbytes\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcallback)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[0;32m    330\u001b[0m         train_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq,\n\u001b[0;32m    331\u001b[0m         action_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise,\n\u001b[0;32m    332\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    333\u001b[0m         learning_starts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_starts,\n\u001b[0;32m    334\u001b[0m         replay_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer,\n\u001b[0;32m    335\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    336\u001b[0m     )\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:568\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    566\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_step()\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mTrainAndLoggingCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls))\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(model_path)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:844\u001b[0m, in \u001b[0;36mBaseAlgorithm.save\u001b[1;34m(self, path, exclude, include)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# Build dict of state_dicts\u001b[39;00m\n\u001b[0;32m    842\u001b[0m params_to_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m--> 844\u001b[0m save_to_zip_file(path, data\u001b[38;5;241m=\u001b[39mdata, params\u001b[38;5;241m=\u001b[39mparams_to_save, pytorch_variables\u001b[38;5;241m=\u001b[39mpytorch_variables)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:329\u001b[0m, in \u001b[0;36msave_to_zip_file\u001b[1;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name, dict_ \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m archive\u001b[38;5;241m.\u001b[39mopen(file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_zip64\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_file:\n\u001b[1;32m--> 329\u001b[0m             th\u001b[38;5;241m.\u001b[39msave(dict_, param_file)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Save metadata: library version when file was saved\u001b[39;00m\n\u001b[0;32m    331\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_stable_baselines3_version\u001b[39m\u001b[38;5;124m\"\u001b[39m, sb3\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:491\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_like\u001b[38;5;241m.\u001b[39mwrite_end_of_file()\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 456852352 vs 456852304"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1ec7e0d12d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(os.path.join('train', 'best_model_4700')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m----> 9\u001b[0m obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[0;32m     10\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     11\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mMazeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     17\u001b[0m     action_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     22\u001b[0m     }\n\u001b[1;32m---> 24\u001b[0m     pydirectinput\u001b[38;5;241m.\u001b[39mpress(action_map[action])\n\u001b[0;32m     26\u001b[0m     done, done_cap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_done()\n\u001b[0;32m     27\u001b[0m     new_observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pydirectinput\\__init__.py:242\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m funcArgs \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetcallargs(wrappedFunction, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[1;32m--> 242\u001b[0m returnVal \u001b[38;5;241m=\u001b[39m wrappedFunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m _handlePause(funcArgs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pause\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pydirectinput\\__init__.py:544\u001b[0m, in \u001b[0;36mpress\u001b[1;34m(keys, presses, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m    543\u001b[0m     failSafeCheck()\n\u001b[1;32m--> 544\u001b[0m     downed \u001b[38;5;241m=\u001b[39m keyDown(k)\n\u001b[0;32m    545\u001b[0m     upped \u001b[38;5;241m=\u001b[39m keyUp(k)\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;66;03m# Count key press as complete if key was \"downed\" and \"upped\" successfully\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[38;5;241m=\u001b[39m wrappedFunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pause\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(1): \n",
    "    obs = env.reset()\n",
    "    obs = obs[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        print(action)\n",
    "        obs, reward, done, _, _ = env.step(int(action))\n",
    "        time.sleep(0.01)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
